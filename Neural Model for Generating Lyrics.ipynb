{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project uses LSTMs in Tensorflow Keras to build word based language models for the song lyrics by a chosen author (I'm going to try The Beatles).\n",
    "\n",
    "The lyrics come from a [Kaggle dataset](https://www.kaggle.com/mousehead/songlyrics) of lyrics scraped from lyricsfreak.com. \n",
    "\n",
    "The model will have two layers of LSTMs and we will try generating text after various levels of training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as tk\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text  \n",
       "0  Look at her face, it's a wonderful face  \\nAnd...  \n",
       "1  Take it easy with me, please  \\nTouch me gentl...  \n",
       "2  I'll never know why I had to go  \\nWhy I had t...  \n",
       "3  Making somebody happy is a question of give an...  \n",
       "4  Making somebody happy is a question of give an...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('songdata.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a function to extract the songs from a specific artist\n",
    "def get_songs(artist, df):\n",
    "    songs = df['text'][df['artist']==artist]\n",
    "    return songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n"
     ]
    }
   ],
   "source": [
    "# Testing above function\n",
    "print(len(get_songs('The Beatles', df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to preserve the new lines, and these punctuation: .,?!() as they seem relevant to many lyrics\n",
    "def reformat_song(song):\n",
    "    song = song.replace('\\n', ' \\n ').replace('.','\\.').replace(',', '\\,').replace('?','\\?').replace('!','\\!').replace('(', '( ').replace(')', ' )')\n",
    "    return song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\\, is a test \n",
      " for sure\\! ( yeah\\? )\n"
     ]
    }
   ],
   "source": [
    "test_song = 'this, is a test\\nfor sure! (yeah?)'\n",
    "print(reformat_song(test_song))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_songs(series):\n",
    "    reformatted = series.apply(reformat_song)\n",
    "    return reformatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178,)\n",
      "1198    Well\\, if your hands start a-clappin'   \\n And...\n",
      "1199    Words are flowing out like   \\n Endless rain i...\n",
      "1200    Whenever I want you around\\, yeah   \\n All I g...\n",
      "1201    I give her all my love   \\n That's all I do   ...\n",
      "1202    You tell me that you've got everything you wan...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# these are the beatles songs which we will use for our example\n",
    "beatles_songs = reformat_songs(get_songs('The Beatles', df))\n",
    "print(beatles_songs.shape)\n",
    "print(beatles_songs.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform data for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 2361\n"
     ]
    }
   ],
   "source": [
    "# we need to build a vocabulary and I want to include new lines, and this punctuation: .,?!-()\n",
    "default_filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "to_filter = default_filters.replace('\\n','').replace('.','').replace(',', '').replace('!','').replace('?','').replace('-','').replace('(','').replace(')','').replace(\"''\", '')\n",
    "tokenizer = Tokenizer(filters=to_filter)\n",
    "tokenizer.fit_on_texts(beatles_songs)\n",
    "vocab = set(tokenizer.word_index.keys())\n",
    "vocab_size = len(vocab)\n",
    "print('Vocab Size:', vocab_size)\n",
    "vocab_dim = vocab_size+1 # our ML algorithms will need an additional index because no words will get mapped to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['log', 'maid', 'oww', 'portuguese', 'hanging', 'king', 'her', 'ye-ye-yeh', 'united', \"orphan's\", 'besame', 'would', 'man', 'seems', 'dreams', 'spoken', 'your', 'teaser', 'innocence', 'doors']\n"
     ]
    }
   ],
   "source": [
    "print(list(vocab)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# producing sequences of max_length words with 1 output word\n",
    "def gen_Xy(songs, tokenizer, max_length):\n",
    "    sequences = []\n",
    "    for song in songs:\n",
    "        # encode words to integer values\n",
    "        encoded = tokenizer.texts_to_sequences([song])[0]\n",
    "        # generate sequences of length max_length + 1 to produce input and output values\n",
    "        for i in range(max_length, len(encoded)-1):\n",
    "            seq = encoded[i-max_length:i+1]\n",
    "            sequences.append(seq)\n",
    "    sequences = np.array(sequences)\n",
    "    X, y = sequences[:,:-1], sequences[:,-1]\n",
    "    assert(X.shape[1]==max_length)\n",
    "    y = to_categorical(y, num_classes=(len(tokenizer.word_index)+1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39241, 5) (39241, 2362)\n"
     ]
    }
   ],
   "source": [
    "# Producing my X and y matrices\n",
    "max_length = 5\n",
    "X, y = gen_Xy(beatles_songs, tokenizer, 5)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 5, 32)             75584     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 5, 64)             24832     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2362)              153530    \n",
      "=================================================================\n",
      "Total params: 291,130\n",
      "Trainable params: 291,130\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "#Model with an embedding layer of 50 nodes, and two LSTM layers of 64 nodes\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_dim, 32, input_length=max_length))\n",
    "model.add(LSTM(64, dropout=.2, return_sequences=True))\n",
    "model.add(LSTM(64, dropout=.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(vocab_dim, activation='softmax'))\n",
    "    \n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "model.summary()\n",
    "plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model and Test Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this generates text from seed text using the model \n",
    "def gen_text(model, tokenizer, seed_text, max_length, n_words):\n",
    "    ix_to_words = dict([(i, c) for c, i in tokenizer.word_index.items()])\n",
    "    text = seed_text\n",
    "    for _ in range(n_words):\n",
    "        encoded = tokenizer.texts_to_sequences([text])[0]\n",
    "        padded = pad_sequences([encoded], maxlen=max_length, truncating='pre')\n",
    "        y_hat = model.predict_classes(padded, verbose=0)\n",
    "        new_word = ix_to_words[int(y_hat)]\n",
    "        text += ' ' + new_word\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is to generate random seed text for the above function\n",
    "def gen_rand_seq(X, tokenizer):\n",
    "    ix_to_words = dict([(i, c) for c, i in tokenizer.word_index.items()])\n",
    "    random.seed=123\n",
    "    index = random.randrange(len(X))\n",
    "    seq = X[index]\n",
    "    words = [ix_to_words[i] for i in seq]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bay-ee-a-by ) \n",
      " yay , junior 'you religion biding biding outside outside mm-mm-mm-di-di-di nineteen contempt\n"
     ]
    }
   ],
   "source": [
    "# no training\n",
    "seed_text = gen_rand_seq(X, tokenizer)\n",
    "print(gen_text(model, tokenizer, seed_text, max_length, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with no training, the output is vaguely Beatlesesque because of the limited vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39241 samples\n",
      "Epoch 1/10\n",
      "   64/39241 [..............................] - ETA: 1:21:03 - loss: 7.7672 - accuracy: 0.0000e+00WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.628713). Check your callbacks.\n",
      "39241/39241 [==============================] - 15s 377us/sample - loss: 5.5989 - accuracy: 0.1476\n",
      "Epoch 2/10\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 5.3189 - accuracy: 0.1481\n",
      "Epoch 3/10\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 5.1326 - accuracy: 0.1493\n",
      "Epoch 4/10\n",
      "39241/39241 [==============================] - 7s 177us/sample - loss: 4.8481 - accuracy: 0.1678\n",
      "Epoch 5/10\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 4.5845 - accuracy: 0.1860\n",
      "Epoch 6/10\n",
      "39241/39241 [==============================] - 7s 182us/sample - loss: 4.3995 - accuracy: 0.1980\n",
      "Epoch 7/10\n",
      "39241/39241 [==============================] - 7s 183us/sample - loss: 4.2553 - accuracy: 0.2049\n",
      "Epoch 8/10\n",
      "39241/39241 [==============================] - 7s 183us/sample - loss: 4.1270 - accuracy: 0.2142\n",
      "Epoch 9/10\n",
      "39241/39241 [==============================] - 8s 192us/sample - loss: 4.0105 - accuracy: 0.2225\n",
      "Epoch 10/10\n",
      "39241/39241 [==============================] - 7s 182us/sample - loss: 3.9039 - accuracy: 0.2321\n",
      "see how they smile like the way \n",
      " \n",
      " i got a way \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First ten epochs of training\n",
    "\n",
    "#TB callback\n",
    "log_dir = 'logs/fit/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "tb_callback = tk.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# fit model for 10 epochs\n",
    "model.fit(X, y, batch_size=64, epochs=10, callbacks=[tb_callback])\n",
    "# save model\n",
    "model.save('model.h5')\n",
    "\n",
    "# generate some text\n",
    "seed_text = gen_rand_seq(X, tokenizer)\n",
    "print(gen_text(model, tokenizer, seed_text, max_length, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 10 epochs we're getting some new lines and some repetition (which is vaguely lyrical):\n",
    "```\n",
    "see how they smile like the way \n",
    "\n",
    "i got a way\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39241 samples\n",
      "Epoch 1/20\n",
      "39241/39241 [==============================] - 7s 179us/sample - loss: 3.8045 - accuracy: 0.2403\n",
      "Epoch 2/20\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 3.7076 - accuracy: 0.2507\n",
      "Epoch 3/20\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 3.6149 - accuracy: 0.2578\n",
      "Epoch 4/20\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 3.5314 - accuracy: 0.2655\n",
      "Epoch 5/20\n",
      "39241/39241 [==============================] - 7s 171us/sample - loss: 3.4431 - accuracy: 0.2741\n",
      "Epoch 6/20\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 3.3703 - accuracy: 0.2819\n",
      "Epoch 7/20\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 3.3001 - accuracy: 0.2907\n",
      "Epoch 8/20\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 3.2283 - accuracy: 0.3009\n",
      "Epoch 9/20\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 3.1655 - accuracy: 0.3085\n",
      "Epoch 10/20\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 3.1054 - accuracy: 0.3183\n",
      "Epoch 11/20\n",
      "39241/39241 [==============================] - 7s 176us/sample - loss: 3.0529 - accuracy: 0.3223\n",
      "Epoch 12/20\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 2.9913 - accuracy: 0.3325\n",
      "Epoch 13/20\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 2.9538 - accuracy: 0.3380\n",
      "Epoch 14/20\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 2.8975 - accuracy: 0.3462\n",
      "Epoch 15/20\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 2.8541 - accuracy: 0.3536\n",
      "Epoch 16/20\n",
      "39241/39241 [==============================] - 7s 178us/sample - loss: 2.8181 - accuracy: 0.3591\n",
      "Epoch 17/20\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 2.7763 - accuracy: 0.3682\n",
      "Epoch 18/20\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 2.7470 - accuracy: 0.3701\n",
      "Epoch 19/20\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 2.7053 - accuracy: 0.3798\n",
      "Epoch 20/20\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 2.6744 - accuracy: 0.3829\n",
      "hide it in a hiding in the garden of the loved \n",
      " and maybe it\n"
     ]
    }
   ],
   "source": [
    "# Next 20 epochs of training\n",
    "\n",
    "#TB callback\n",
    "log_dir = 'logs/fit/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "tb_callback = tk.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# fit model for 20 epochs\n",
    "model.fit(X, y, batch_size=64, epochs=20, callbacks=[tb_callback])\n",
    "# save model\n",
    "model.save('model.h5')\n",
    "\n",
    "# generate some text\n",
    "seed_text = gen_rand_seq(X, tokenizer)\n",
    "print(gen_text(model, tokenizer, seed_text, max_length, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After another 20 epochs (30 total) we're getting more sensible sentences:\n",
    "```\n",
    "hide it in a hiding in the garden of the loved \n",
    " and maybe it\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39241 samples\n",
      "Epoch 1/30\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 2.6475 - accuracy: 0.3876\n",
      "Epoch 2/30\n",
      "39241/39241 [==============================] - 7s 171us/sample - loss: 2.6113 - accuracy: 0.3945\n",
      "Epoch 3/30\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 2.5854 - accuracy: 0.3953\n",
      "Epoch 4/30\n",
      "39241/39241 [==============================] - 7s 186us/sample - loss: 2.5559 - accuracy: 0.4050\n",
      "Epoch 5/30\n",
      "39241/39241 [==============================] - 7s 185us/sample - loss: 2.5301 - accuracy: 0.4073\n",
      "Epoch 6/30\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 2.5016 - accuracy: 0.4117\n",
      "Epoch 7/30\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 2.4830 - accuracy: 0.4173\n",
      "Epoch 8/30\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 2.4508 - accuracy: 0.4192\n",
      "Epoch 9/30\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 2.4368 - accuracy: 0.4216\n",
      "Epoch 10/30\n",
      "39241/39241 [==============================] - 7s 177us/sample - loss: 2.4106 - accuracy: 0.4284\n",
      "Epoch 11/30\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 2.3877 - accuracy: 0.4325\n",
      "Epoch 12/30\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 2.3699 - accuracy: 0.4355\n",
      "Epoch 13/30\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 2.3463 - accuracy: 0.4403\n",
      "Epoch 14/30\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 2.3246 - accuracy: 0.4429\n",
      "Epoch 15/30\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 2.3104 - accuracy: 0.4477\n",
      "Epoch 16/30\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 2.2950 - accuracy: 0.4496\n",
      "Epoch 17/30\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 2.2741 - accuracy: 0.4528\n",
      "Epoch 18/30\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 2.2581 - accuracy: 0.4556\n",
      "Epoch 19/30\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 2.2377 - accuracy: 0.4607\n",
      "Epoch 20/30\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 2.2183 - accuracy: 0.4651\n",
      "Epoch 21/30\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 2.1985 - accuracy: 0.4709\n",
      "Epoch 22/30\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 2.1920 - accuracy: 0.4685\n",
      "Epoch 23/30\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 2.1690 - accuracy: 0.4704\n",
      "Epoch 24/30\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 2.1547 - accuracy: 0.4769\n",
      "Epoch 25/30\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 2.1432 - accuracy: 0.4789\n",
      "Epoch 26/30\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 2.1308 - accuracy: 0.4803\n",
      "Epoch 27/30\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 2.1217 - accuracy: 0.4841\n",
      "Epoch 28/30\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 2.1098 - accuracy: 0.4844\n",
      "Epoch 29/30\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 2.0981 - accuracy: 0.4875\n",
      "Epoch 30/30\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 2.0726 - accuracy: 0.4930\n",
      "you're missing \n",
      " nowhere man , yeah ) \n",
      " \n",
      " well , i talk about\n"
     ]
    }
   ],
   "source": [
    "# Next 30 epochs of training\n",
    "\n",
    "#TB callback\n",
    "log_dir = 'logs/fit/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "tb_callback = tk.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# fit model for 30 epochs\n",
    "model.fit(X, y, batch_size=64, epochs=30, callbacks=[tb_callback])\n",
    "# save model\n",
    "model.save('model.h5')\n",
    "\n",
    "# generate some text\n",
    "seed_text = gen_rand_seq(X, tokenizer)\n",
    "print(gen_text(model, tokenizer, seed_text, max_length, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After another 30 epochs of training (60 total) we seem to be getting some rhythm: \n",
    "```\n",
    "you're missing \n",
    " nowhere man , yeah ) \n",
    " \n",
    " well , i talk about\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39241 samples\n",
      "Epoch 1/40\n",
      "39241/39241 [==============================] - 7s 185us/sample - loss: 2.0748 - accuracy: 0.4934\n",
      "Epoch 2/40\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 2.0546 - accuracy: 0.4950\n",
      "Epoch 3/40\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 2.0491 - accuracy: 0.4963\n",
      "Epoch 4/40\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 2.0386 - accuracy: 0.4967\n",
      "Epoch 5/40\n",
      "39241/39241 [==============================] - 7s 176us/sample - loss: 2.0329 - accuracy: 0.5014\n",
      "Epoch 6/40\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 2.0229 - accuracy: 0.5022\n",
      "Epoch 7/40\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 2.0038 - accuracy: 0.5087\n",
      "Epoch 8/40\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.9875 - accuracy: 0.5083\n",
      "Epoch 9/40\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.9815 - accuracy: 0.5128\n",
      "Epoch 10/40\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.9702 - accuracy: 0.5150\n",
      "Epoch 11/40\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.9677 - accuracy: 0.5144\n",
      "Epoch 12/40\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.9603 - accuracy: 0.5149\n",
      "Epoch 13/40\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.9419 - accuracy: 0.5198\n",
      "Epoch 14/40\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.9343 - accuracy: 0.5230\n",
      "Epoch 15/40\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.9187 - accuracy: 0.5210\n",
      "Epoch 16/40\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.9143 - accuracy: 0.5219\n",
      "Epoch 17/40\n",
      "39241/39241 [==============================] - 7s 179us/sample - loss: 1.9150 - accuracy: 0.5218\n",
      "Epoch 18/40\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.8998 - accuracy: 0.5275\n",
      "Epoch 19/40\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.8887 - accuracy: 0.5300\n",
      "Epoch 20/40\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.8825 - accuracy: 0.5294\n",
      "Epoch 21/40\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.8745 - accuracy: 0.5338\n",
      "Epoch 22/40\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.8629 - accuracy: 0.5350\n",
      "Epoch 23/40\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.8624 - accuracy: 0.5340\n",
      "Epoch 24/40\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.8469 - accuracy: 0.5406\n",
      "Epoch 25/40\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.8495 - accuracy: 0.5355\n",
      "Epoch 26/40\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.8369 - accuracy: 0.5395\n",
      "Epoch 27/40\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.8249 - accuracy: 0.5437\n",
      "Epoch 28/40\n",
      "39241/39241 [==============================] - 7s 180us/sample - loss: 1.8235 - accuracy: 0.5437\n",
      "Epoch 29/40\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.8192 - accuracy: 0.5445\n",
      "Epoch 30/40\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.8112 - accuracy: 0.5432\n",
      "Epoch 31/40\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.7926 - accuracy: 0.5506\n",
      "Epoch 32/40\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.7910 - accuracy: 0.5501\n",
      "Epoch 33/40\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.7898 - accuracy: 0.5522\n",
      "Epoch 34/40\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.7846 - accuracy: 0.5512\n",
      "Epoch 35/40\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.7711 - accuracy: 0.5526\n",
      "Epoch 36/40\n",
      "39241/39241 [==============================] - 7s 171us/sample - loss: 1.7696 - accuracy: 0.5523\n",
      "Epoch 37/40\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.7597 - accuracy: 0.5548\n",
      "Epoch 38/40\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.7567 - accuracy: 0.5557\n",
      "Epoch 39/40\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.7470 - accuracy: 0.5573\n",
      "Epoch 40/40\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.7465 - accuracy: 0.5568\n",
      ", \n",
      " well , i talk about boys , \n",
      " don't ya know i mean\n"
     ]
    }
   ],
   "source": [
    "# Next 40 epochs of training\n",
    "\n",
    "#TB callback\n",
    "log_dir = 'logs/fit/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "tb_callback = tk.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# fit model for 40 epochs\n",
    "model.fit(X, y, batch_size=64, epochs=40, callbacks=[tb_callback])\n",
    "# save model\n",
    "model.save('model.h5')\n",
    "\n",
    "# generate some text\n",
    "seed_text = gen_rand_seq(X, tokenizer)\n",
    "print(gen_text(model, tokenizer, seed_text, max_length, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After another 40 epochs of training (100 total) we've got more consistent length of lines and the lines seem to relate: \n",
    "```\n",
    ", \n",
    " well , i talk about boys , \n",
    " don't ya know i mean\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39241 samples\n",
      "Epoch 1/100\n",
      "39241/39241 [==============================] - 7s 176us/sample - loss: 1.7300 - accuracy: 0.5601\n",
      "Epoch 2/100\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.7352 - accuracy: 0.5601\n",
      "Epoch 3/100\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.7241 - accuracy: 0.5656\n",
      "Epoch 4/100\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.7218 - accuracy: 0.5625\n",
      "Epoch 5/100\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.7185 - accuracy: 0.5650\n",
      "Epoch 6/100\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.7153 - accuracy: 0.5663\n",
      "Epoch 7/100\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.7007 - accuracy: 0.5693\n",
      "Epoch 8/100\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.7009 - accuracy: 0.5661\n",
      "Epoch 9/100\n",
      "39241/39241 [==============================] - 7s 177us/sample - loss: 1.6961 - accuracy: 0.5667\n",
      "Epoch 10/100\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.6951 - accuracy: 0.5696\n",
      "Epoch 11/100\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.6828 - accuracy: 0.5707\n",
      "Epoch 12/100\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.6821 - accuracy: 0.5714\n",
      "Epoch 13/100\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.6720 - accuracy: 0.5730\n",
      "Epoch 14/100\n",
      "39241/39241 [==============================] - 7s 181us/sample - loss: 1.6776 - accuracy: 0.5759\n",
      "Epoch 15/100\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.6722 - accuracy: 0.5730\n",
      "Epoch 16/100\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.6621 - accuracy: 0.5743\n",
      "Epoch 17/100\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.6616 - accuracy: 0.5747\n",
      "Epoch 18/100\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.6494 - accuracy: 0.5780\n",
      "Epoch 19/100\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.6574 - accuracy: 0.5760\n",
      "Epoch 20/100\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.6493 - accuracy: 0.5769\n",
      "Epoch 21/100\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.6429 - accuracy: 0.5770\n",
      "Epoch 22/100\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.6441 - accuracy: 0.5802\n",
      "Epoch 23/100\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.6423 - accuracy: 0.5806\n",
      "Epoch 24/100\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.6297 - accuracy: 0.5821\n",
      "Epoch 25/100\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.6212 - accuracy: 0.5866\n",
      "Epoch 26/100\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.6184 - accuracy: 0.5826\n",
      "Epoch 27/100\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.6101 - accuracy: 0.5845\n",
      "Epoch 28/100\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.6135 - accuracy: 0.5833\n",
      "Epoch 29/100\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.6020 - accuracy: 0.5889\n",
      "Epoch 30/100\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.6023 - accuracy: 0.5886\n",
      "Epoch 31/100\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.5982 - accuracy: 0.5904\n",
      "Epoch 32/100\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.6004 - accuracy: 0.5874\n",
      "Epoch 33/100\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.5879 - accuracy: 0.5911\n",
      "Epoch 34/100\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.5852 - accuracy: 0.5896\n",
      "Epoch 35/100\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.5789 - accuracy: 0.5911\n",
      "Epoch 36/100\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.5780 - accuracy: 0.5911\n",
      "Epoch 37/100\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.5773 - accuracy: 0.5923\n",
      "Epoch 38/100\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.5715 - accuracy: 0.5936\n",
      "Epoch 39/100\n",
      "39241/39241 [==============================] - 7s 171us/sample - loss: 1.5666 - accuracy: 0.5957\n",
      "Epoch 40/100\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.5732 - accuracy: 0.5921\n",
      "Epoch 41/100\n",
      "39241/39241 [==============================] - 7s 178us/sample - loss: 1.5677 - accuracy: 0.5950\n",
      "Epoch 42/100\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.5577 - accuracy: 0.5952\n",
      "Epoch 43/100\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.5564 - accuracy: 0.5965\n",
      "Epoch 44/100\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.5527 - accuracy: 0.5970\n",
      "Epoch 45/100\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.5548 - accuracy: 0.5972\n",
      "Epoch 46/100\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.5450 - accuracy: 0.6009\n",
      "Epoch 47/100\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.5404 - accuracy: 0.5953\n",
      "Epoch 48/100\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.5398 - accuracy: 0.6012\n",
      "Epoch 49/100\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.5431 - accuracy: 0.5986\n",
      "Epoch 50/100\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.5355 - accuracy: 0.6013\n",
      "Epoch 51/100\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.5343 - accuracy: 0.5988\n",
      "Epoch 52/100\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.5291 - accuracy: 0.6012\n",
      "Epoch 53/100\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.5224 - accuracy: 0.6030\n",
      "Epoch 54/100\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.5168 - accuracy: 0.6062\n",
      "Epoch 55/100\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.5158 - accuracy: 0.6038\n",
      "Epoch 56/100\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.5204 - accuracy: 0.6058\n",
      "Epoch 57/100\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.5095 - accuracy: 0.6085\n",
      "Epoch 58/100\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.5110 - accuracy: 0.6067\n",
      "Epoch 59/100\n",
      "39241/39241 [==============================] - 7s 176us/sample - loss: 1.5134 - accuracy: 0.6056\n",
      "Epoch 60/100\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.4980 - accuracy: 0.6103\n",
      "Epoch 61/100\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.4981 - accuracy: 0.6079\n",
      "Epoch 62/100\n",
      "39241/39241 [==============================] - 7s 178us/sample - loss: 1.4927 - accuracy: 0.6071\n",
      "Epoch 63/100\n",
      "39241/39241 [==============================] - 7s 178us/sample - loss: 1.4994 - accuracy: 0.6098\n",
      "Epoch 64/100\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.4876 - accuracy: 0.6118\n",
      "Epoch 65/100\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.4952 - accuracy: 0.6122\n",
      "Epoch 66/100\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.4931 - accuracy: 0.6089\n",
      "Epoch 67/100\n",
      "39241/39241 [==============================] - 7s 184us/sample - loss: 1.4934 - accuracy: 0.6117\n",
      "Epoch 68/100\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.4885 - accuracy: 0.6132\n",
      "Epoch 69/100\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.4876 - accuracy: 0.6093\n",
      "Epoch 70/100\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.4775 - accuracy: 0.6131\n",
      "Epoch 71/100\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.4797 - accuracy: 0.6115\n",
      "Epoch 72/100\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.4702 - accuracy: 0.6149\n",
      "Epoch 73/100\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.4741 - accuracy: 0.6151\n",
      "Epoch 74/100\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.4647 - accuracy: 0.6167\n",
      "Epoch 75/100\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.4676 - accuracy: 0.6174\n",
      "Epoch 76/100\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.4594 - accuracy: 0.6172\n",
      "Epoch 77/100\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.4638 - accuracy: 0.6191\n",
      "Epoch 78/100\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.4567 - accuracy: 0.6185\n",
      "Epoch 79/100\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.4639 - accuracy: 0.6197\n",
      "Epoch 80/100\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.4595 - accuracy: 0.6146\n",
      "Epoch 81/100\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.4488 - accuracy: 0.6194\n",
      "Epoch 82/100\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.4460 - accuracy: 0.6198\n",
      "Epoch 83/100\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.4596 - accuracy: 0.6142\n",
      "Epoch 84/100\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.4464 - accuracy: 0.6205\n",
      "Epoch 85/100\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.4488 - accuracy: 0.6221\n",
      "Epoch 86/100\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.4379 - accuracy: 0.6210\n",
      "Epoch 87/100\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.4392 - accuracy: 0.6204\n",
      "Epoch 88/100\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.4367 - accuracy: 0.6222\n",
      "Epoch 89/100\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.4298 - accuracy: 0.6218\n",
      "Epoch 90/100\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.4382 - accuracy: 0.6219\n",
      "Epoch 91/100\n",
      "39241/39241 [==============================] - 7s 176us/sample - loss: 1.4276 - accuracy: 0.6233\n",
      "Epoch 92/100\n",
      "39241/39241 [==============================] - 7s 176us/sample - loss: 1.4226 - accuracy: 0.6267\n",
      "Epoch 93/100\n",
      "39241/39241 [==============================] - 7s 181us/sample - loss: 1.4345 - accuracy: 0.6221\n",
      "Epoch 94/100\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.4221 - accuracy: 0.6235\n",
      "Epoch 95/100\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.4211 - accuracy: 0.6271\n",
      "Epoch 96/100\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.4210 - accuracy: 0.6244\n",
      "Epoch 97/100\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.4089 - accuracy: 0.6257\n",
      "Epoch 98/100\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.4053 - accuracy: 0.6284\n",
      "Epoch 99/100\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.4055 - accuracy: 0.6292\n",
      "Epoch 100/100\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.4094 - accuracy: 0.6271\n",
      "nations , \n",
      " congratulations . \n",
      " \n",
      " all we are saying is give peace a\n"
     ]
    }
   ],
   "source": [
    "# Next 100 epochs of training\n",
    "\n",
    "#TB callback\n",
    "log_dir = 'logs/fit/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "tb_callback = tk.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# fit model for 100 epochs\n",
    "model.fit(X, y, batch_size=64, epochs=100, callbacks=[tb_callback])\n",
    "# save model\n",
    "model.save('model.h5')\n",
    "\n",
    "# generate some text\n",
    "seed_text = gen_rand_seq(X, tokenizer)\n",
    "print(gen_text(model, tokenizer, seed_text, max_length, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After another 100 epochs of training (200 total):\n",
    "\n",
    "```\n",
    "nations , \n",
    " congratulations . \n",
    " \n",
    " all we are saying is give peace a\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39241 samples\n",
      "Epoch 1/200\n",
      "39241/39241 [==============================] - 7s 176us/sample - loss: 1.4081 - accuracy: 0.6275\n",
      "Epoch 2/200\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.4068 - accuracy: 0.6284\n",
      "Epoch 3/200\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.4098 - accuracy: 0.6292\n",
      "Epoch 4/200\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.4131 - accuracy: 0.6278\n",
      "Epoch 5/200\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.4063 - accuracy: 0.6287\n",
      "Epoch 6/200\n",
      "39241/39241 [==============================] - 7s 184us/sample - loss: 1.4016 - accuracy: 0.6285\n",
      "Epoch 7/200\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.3951 - accuracy: 0.6298\n",
      "Epoch 8/200\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.3970 - accuracy: 0.6344\n",
      "Epoch 9/200\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.3917 - accuracy: 0.6335\n",
      "Epoch 10/200\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.3938 - accuracy: 0.6313\n",
      "Epoch 11/200\n",
      "39241/39241 [==============================] - 7s 171us/sample - loss: 1.3902 - accuracy: 0.6306\n",
      "Epoch 12/200\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.3880 - accuracy: 0.6341\n",
      "Epoch 13/200\n",
      "39241/39241 [==============================] - 7s 179us/sample - loss: 1.3869 - accuracy: 0.6354\n",
      "Epoch 14/200\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.3763 - accuracy: 0.6346\n",
      "Epoch 15/200\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.3848 - accuracy: 0.6324\n",
      "Epoch 16/200\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.3869 - accuracy: 0.6343\n",
      "Epoch 17/200\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.3827 - accuracy: 0.6312\n",
      "Epoch 18/200\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.3794 - accuracy: 0.6340\n",
      "Epoch 19/200\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.3789 - accuracy: 0.6365\n",
      "Epoch 20/200\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.3703 - accuracy: 0.6358\n",
      "Epoch 21/200\n",
      "39241/39241 [==============================] - 7s 171us/sample - loss: 1.3794 - accuracy: 0.6360\n",
      "Epoch 22/200\n",
      "39241/39241 [==============================] - 7s 171us/sample - loss: 1.3833 - accuracy: 0.6303\n",
      "Epoch 23/200\n",
      "39241/39241 [==============================] - 7s 171us/sample - loss: 1.3730 - accuracy: 0.6348\n",
      "Epoch 24/200\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.3669 - accuracy: 0.6365\n",
      "Epoch 25/200\n",
      "39241/39241 [==============================] - 7s 171us/sample - loss: 1.3695 - accuracy: 0.6375\n",
      "Epoch 26/200\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.3622 - accuracy: 0.6391\n",
      "Epoch 27/200\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.3735 - accuracy: 0.6376\n",
      "Epoch 28/200\n",
      "39241/39241 [==============================] - 7s 171us/sample - loss: 1.3537 - accuracy: 0.6390\n",
      "Epoch 29/200\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.3689 - accuracy: 0.6364\n",
      "Epoch 30/200\n",
      "39241/39241 [==============================] - 7s 171us/sample - loss: 1.3546 - accuracy: 0.6395\n",
      "Epoch 31/200\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.3551 - accuracy: 0.6399\n",
      "Epoch 32/200\n",
      "39241/39241 [==============================] - 7s 177us/sample - loss: 1.3587 - accuracy: 0.6406\n",
      "Epoch 33/200\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.3482 - accuracy: 0.6404\n",
      "Epoch 34/200\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.3485 - accuracy: 0.6425\n",
      "Epoch 35/200\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.3511 - accuracy: 0.6427\n",
      "Epoch 36/200\n",
      "39241/39241 [==============================] - 7s 177us/sample - loss: 1.3468 - accuracy: 0.6394\n",
      "Epoch 37/200\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.3507 - accuracy: 0.6386\n",
      "Epoch 38/200\n",
      "39241/39241 [==============================] - 7s 179us/sample - loss: 1.3464 - accuracy: 0.6434\n",
      "Epoch 39/200\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.3524 - accuracy: 0.6404\n",
      "Epoch 40/200\n",
      "39241/39241 [==============================] - 7s 176us/sample - loss: 1.3334 - accuracy: 0.6431\n",
      "Epoch 41/200\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.3425 - accuracy: 0.6431\n",
      "Epoch 42/200\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.3532 - accuracy: 0.6406\n",
      "Epoch 43/200\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.3341 - accuracy: 0.6417\n",
      "Epoch 44/200\n",
      "39241/39241 [==============================] - 7s 178us/sample - loss: 1.3409 - accuracy: 0.6438\n",
      "Epoch 45/200\n",
      "39241/39241 [==============================] - 7s 179us/sample - loss: 1.3298 - accuracy: 0.6449\n",
      "Epoch 46/200\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.3333 - accuracy: 0.6437\n",
      "Epoch 47/200\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.3330 - accuracy: 0.6446\n",
      "Epoch 48/200\n",
      "39241/39241 [==============================] - 7s 185us/sample - loss: 1.3242 - accuracy: 0.6464\n",
      "Epoch 49/200\n",
      "39241/39241 [==============================] - 7s 177us/sample - loss: 1.3358 - accuracy: 0.6442\n",
      "Epoch 50/200\n",
      "39241/39241 [==============================] - 7s 177us/sample - loss: 1.3198 - accuracy: 0.6484\n",
      "Epoch 51/200\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.3230 - accuracy: 0.6470\n",
      "Epoch 52/200\n",
      "39241/39241 [==============================] - 7s 176us/sample - loss: 1.3248 - accuracy: 0.6472\n",
      "Epoch 53/200\n",
      "39241/39241 [==============================] - 7s 181us/sample - loss: 1.3207 - accuracy: 0.6467\n",
      "Epoch 54/200\n",
      "39241/39241 [==============================] - 7s 178us/sample - loss: 1.3160 - accuracy: 0.6497\n",
      "Epoch 55/200\n",
      "39241/39241 [==============================] - 7s 179us/sample - loss: 1.3221 - accuracy: 0.6446\n",
      "Epoch 56/200\n",
      "39241/39241 [==============================] - 7s 176us/sample - loss: 1.3186 - accuracy: 0.6460\n",
      "Epoch 57/200\n",
      "39241/39241 [==============================] - 7s 177us/sample - loss: 1.3153 - accuracy: 0.6456\n",
      "Epoch 58/200\n",
      "39241/39241 [==============================] - 7s 185us/sample - loss: 1.3210 - accuracy: 0.6449\n",
      "Epoch 59/200\n",
      "39241/39241 [==============================] - 7s 178us/sample - loss: 1.3117 - accuracy: 0.6523\n",
      "Epoch 60/200\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.3178 - accuracy: 0.6477\n",
      "Epoch 61/200\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.3193 - accuracy: 0.6458\n",
      "Epoch 62/200\n",
      "39241/39241 [==============================] - 7s 180us/sample - loss: 1.3133 - accuracy: 0.6487\n",
      "Epoch 63/200\n",
      "39241/39241 [==============================] - 7s 184us/sample - loss: 1.3067 - accuracy: 0.6508\n",
      "Epoch 64/200\n",
      "39241/39241 [==============================] - 8s 196us/sample - loss: 1.3050 - accuracy: 0.6490\n",
      "Epoch 65/200\n",
      "39241/39241 [==============================] - 8s 193us/sample - loss: 1.3119 - accuracy: 0.6476\n",
      "Epoch 66/200\n",
      "39241/39241 [==============================] - 8s 198us/sample - loss: 1.2992 - accuracy: 0.6512\n",
      "Epoch 67/200\n",
      "39241/39241 [==============================] - 7s 190us/sample - loss: 1.3045 - accuracy: 0.6498\n",
      "Epoch 68/200\n",
      "39241/39241 [==============================] - 7s 179us/sample - loss: 1.3069 - accuracy: 0.6491\n",
      "Epoch 69/200\n",
      "39241/39241 [==============================] - 7s 184us/sample - loss: 1.3007 - accuracy: 0.6494\n",
      "Epoch 70/200\n",
      "39241/39241 [==============================] - 7s 179us/sample - loss: 1.3082 - accuracy: 0.6502\n",
      "Epoch 71/200\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.3051 - accuracy: 0.6488\n",
      "Epoch 72/200\n",
      "39241/39241 [==============================] - 7s 177us/sample - loss: 1.3030 - accuracy: 0.6491\n",
      "Epoch 73/200\n",
      "39241/39241 [==============================] - 7s 182us/sample - loss: 1.2946 - accuracy: 0.6527\n",
      "Epoch 74/200\n",
      "39241/39241 [==============================] - 7s 185us/sample - loss: 1.2936 - accuracy: 0.6500\n",
      "Epoch 75/200\n",
      "39241/39241 [==============================] - 7s 183us/sample - loss: 1.3009 - accuracy: 0.6533\n",
      "Epoch 76/200\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.2928 - accuracy: 0.6532\n",
      "Epoch 77/200\n",
      "39241/39241 [==============================] - 7s 177us/sample - loss: 1.2792 - accuracy: 0.6544\n",
      "Epoch 78/200\n",
      "39241/39241 [==============================] - 7s 180us/sample - loss: 1.2970 - accuracy: 0.6508\n",
      "Epoch 79/200\n",
      "39241/39241 [==============================] - 7s 178us/sample - loss: 1.2822 - accuracy: 0.6549\n",
      "Epoch 80/200\n",
      "39241/39241 [==============================] - 7s 177us/sample - loss: 1.2903 - accuracy: 0.6528\n",
      "Epoch 81/200\n",
      "39241/39241 [==============================] - 7s 176us/sample - loss: 1.2868 - accuracy: 0.6551\n",
      "Epoch 82/200\n",
      "39241/39241 [==============================] - 7s 177us/sample - loss: 1.2769 - accuracy: 0.6564\n",
      "Epoch 83/200\n",
      "39241/39241 [==============================] - 7s 178us/sample - loss: 1.2820 - accuracy: 0.6570\n",
      "Epoch 84/200\n",
      "39241/39241 [==============================] - 7s 185us/sample - loss: 1.2736 - accuracy: 0.6573\n",
      "Epoch 85/200\n",
      "39241/39241 [==============================] - 7s 177us/sample - loss: 1.2907 - accuracy: 0.6524\n",
      "Epoch 86/200\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.2877 - accuracy: 0.6538\n",
      "Epoch 87/200\n",
      "39241/39241 [==============================] - 7s 176us/sample - loss: 1.2773 - accuracy: 0.6596\n",
      "Epoch 88/200\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.2799 - accuracy: 0.6540\n",
      "Epoch 89/200\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.2782 - accuracy: 0.6545\n",
      "Epoch 90/200\n",
      "39241/39241 [==============================] - 7s 176us/sample - loss: 1.2832 - accuracy: 0.6548\n",
      "Epoch 91/200\n",
      "39241/39241 [==============================] - 7s 176us/sample - loss: 1.2776 - accuracy: 0.6571\n",
      "Epoch 92/200\n",
      "39241/39241 [==============================] - 7s 176us/sample - loss: 1.2744 - accuracy: 0.6564\n",
      "Epoch 93/200\n",
      "39241/39241 [==============================] - 7s 177us/sample - loss: 1.2711 - accuracy: 0.6550\n",
      "Epoch 94/200\n",
      "39241/39241 [==============================] - 7s 177us/sample - loss: 1.2739 - accuracy: 0.6587\n",
      "Epoch 95/200\n",
      "39241/39241 [==============================] - 7s 178us/sample - loss: 1.2702 - accuracy: 0.6580\n",
      "Epoch 96/200\n",
      "39241/39241 [==============================] - 7s 176us/sample - loss: 1.2669 - accuracy: 0.6550\n",
      "Epoch 97/200\n",
      "39241/39241 [==============================] - 7s 179us/sample - loss: 1.2797 - accuracy: 0.6568\n",
      "Epoch 98/200\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.2659 - accuracy: 0.6545\n",
      "Epoch 99/200\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.2678 - accuracy: 0.6571\n",
      "Epoch 100/200\n",
      "39241/39241 [==============================] - 7s 181us/sample - loss: 1.2703 - accuracy: 0.6568\n",
      "Epoch 101/200\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.2617 - accuracy: 0.6626\n",
      "Epoch 102/200\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.2699 - accuracy: 0.6555\n",
      "Epoch 103/200\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.2628 - accuracy: 0.6598\n",
      "Epoch 104/200\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.2658 - accuracy: 0.6572\n",
      "Epoch 105/200\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.2628 - accuracy: 0.6583\n",
      "Epoch 106/200\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.2588 - accuracy: 0.6605\n",
      "Epoch 107/200\n",
      "39241/39241 [==============================] - 7s 171us/sample - loss: 1.2616 - accuracy: 0.6617\n",
      "Epoch 108/200\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.2538 - accuracy: 0.6602\n",
      "Epoch 109/200\n",
      "39241/39241 [==============================] - 7s 171us/sample - loss: 1.2705 - accuracy: 0.6577\n",
      "Epoch 110/200\n",
      "39241/39241 [==============================] - 7s 182us/sample - loss: 1.2566 - accuracy: 0.6626\n",
      "Epoch 111/200\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.2557 - accuracy: 0.6598\n",
      "Epoch 112/200\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.2660 - accuracy: 0.6585\n",
      "Epoch 113/200\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.2509 - accuracy: 0.6626\n",
      "Epoch 114/200\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.2503 - accuracy: 0.6604\n",
      "Epoch 115/200\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.2516 - accuracy: 0.6615\n",
      "Epoch 116/200\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.2600 - accuracy: 0.6592\n",
      "Epoch 117/200\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.2498 - accuracy: 0.6628\n",
      "Epoch 118/200\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.2467 - accuracy: 0.6618\n",
      "Epoch 119/200\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.2378 - accuracy: 0.6644\n",
      "Epoch 120/200\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.2411 - accuracy: 0.6637\n",
      "Epoch 121/200\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.2524 - accuracy: 0.6628\n",
      "Epoch 122/200\n",
      "39241/39241 [==============================] - 7s 177us/sample - loss: 1.2374 - accuracy: 0.6646\n",
      "Epoch 123/200\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.2476 - accuracy: 0.6605\n",
      "Epoch 124/200\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.2387 - accuracy: 0.6651\n",
      "Epoch 125/200\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.2406 - accuracy: 0.6619\n",
      "Epoch 126/200\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.2444 - accuracy: 0.6658\n",
      "Epoch 127/200\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.2369 - accuracy: 0.6646\n",
      "Epoch 128/200\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.2459 - accuracy: 0.6628\n",
      "Epoch 129/200\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.2437 - accuracy: 0.6609\n",
      "Epoch 130/200\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.2445 - accuracy: 0.6652\n",
      "Epoch 131/200\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.2339 - accuracy: 0.6665\n",
      "Epoch 132/200\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.2418 - accuracy: 0.6643\n",
      "Epoch 133/200\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.2465 - accuracy: 0.6644\n",
      "Epoch 134/200\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.2338 - accuracy: 0.6661\n",
      "Epoch 135/200\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.2261 - accuracy: 0.6654\n",
      "Epoch 136/200\n",
      "39241/39241 [==============================] - 7s 182us/sample - loss: 1.2370 - accuracy: 0.6653\n",
      "Epoch 137/200\n",
      "39241/39241 [==============================] - 7s 177us/sample - loss: 1.2327 - accuracy: 0.6650\n",
      "Epoch 138/200\n",
      "39241/39241 [==============================] - 7s 178us/sample - loss: 1.2264 - accuracy: 0.6694\n",
      "Epoch 139/200\n",
      "39241/39241 [==============================] - 7s 179us/sample - loss: 1.2303 - accuracy: 0.6664\n",
      "Epoch 140/200\n",
      "39241/39241 [==============================] - 7s 184us/sample - loss: 1.2376 - accuracy: 0.6632\n",
      "Epoch 141/200\n",
      "39241/39241 [==============================] - 7s 183us/sample - loss: 1.2254 - accuracy: 0.6671\n",
      "Epoch 142/200\n",
      "39241/39241 [==============================] - 7s 179us/sample - loss: 1.2399 - accuracy: 0.6646\n",
      "Epoch 143/200\n",
      "39241/39241 [==============================] - 7s 177us/sample - loss: 1.2257 - accuracy: 0.6673\n",
      "Epoch 144/200\n",
      "39241/39241 [==============================] - 7s 177us/sample - loss: 1.2247 - accuracy: 0.6661\n",
      "Epoch 145/200\n",
      "39241/39241 [==============================] - 7s 177us/sample - loss: 1.2237 - accuracy: 0.6680\n",
      "Epoch 146/200\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.2345 - accuracy: 0.6663\n",
      "Epoch 147/200\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.2239 - accuracy: 0.6667\n",
      "Epoch 148/200\n",
      "39241/39241 [==============================] - 7s 176us/sample - loss: 1.2291 - accuracy: 0.6668\n",
      "Epoch 149/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.2135 - accuracy: 0.6683\n",
      "Epoch 150/200\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.2236 - accuracy: 0.6672\n",
      "Epoch 151/200\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.2245 - accuracy: 0.6699\n",
      "Epoch 152/200\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.2196 - accuracy: 0.6674\n",
      "Epoch 153/200\n",
      "39241/39241 [==============================] - 7s 176us/sample - loss: 1.2290 - accuracy: 0.6676\n",
      "Epoch 154/200\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.2209 - accuracy: 0.6679\n",
      "Epoch 155/200\n",
      "39241/39241 [==============================] - 7s 172us/sample - loss: 1.2154 - accuracy: 0.6690\n",
      "Epoch 156/200\n",
      "39241/39241 [==============================] - 7s 182us/sample - loss: 1.2150 - accuracy: 0.6699\n",
      "Epoch 157/200\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.2223 - accuracy: 0.6690\n",
      "Epoch 158/200\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.2225 - accuracy: 0.6677\n",
      "Epoch 159/200\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.2073 - accuracy: 0.6714\n",
      "Epoch 160/200\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.2131 - accuracy: 0.6705\n",
      "Epoch 161/200\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.2120 - accuracy: 0.6701\n",
      "Epoch 162/200\n",
      "39241/39241 [==============================] - 7s 184us/sample - loss: 1.2063 - accuracy: 0.6712\n",
      "Epoch 163/200\n",
      "39241/39241 [==============================] - 7s 176us/sample - loss: 1.2082 - accuracy: 0.6701\n",
      "Epoch 164/200\n",
      "39241/39241 [==============================] - 7s 179us/sample - loss: 1.2136 - accuracy: 0.6677\n",
      "Epoch 165/200\n",
      "39241/39241 [==============================] - 7s 177us/sample - loss: 1.2139 - accuracy: 0.6701\n",
      "Epoch 166/200\n",
      "39241/39241 [==============================] - 7s 179us/sample - loss: 1.2056 - accuracy: 0.6709\n",
      "Epoch 167/200\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.2131 - accuracy: 0.6709\n",
      "Epoch 168/200\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.2030 - accuracy: 0.6725\n",
      "Epoch 169/200\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.2087 - accuracy: 0.6706\n",
      "Epoch 170/200\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.2022 - accuracy: 0.6731\n",
      "Epoch 171/200\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.2069 - accuracy: 0.6703\n",
      "Epoch 172/200\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.1999 - accuracy: 0.6725\n",
      "Epoch 173/200\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.1967 - accuracy: 0.6752\n",
      "Epoch 174/200\n",
      "39241/39241 [==============================] - 7s 181us/sample - loss: 1.2107 - accuracy: 0.6697\n",
      "Epoch 175/200\n",
      "39241/39241 [==============================] - 7s 176us/sample - loss: 1.2031 - accuracy: 0.6694\n",
      "Epoch 176/200\n",
      "39241/39241 [==============================] - 7s 176us/sample - loss: 1.2064 - accuracy: 0.6716\n",
      "Epoch 177/200\n",
      "39241/39241 [==============================] - 7s 176us/sample - loss: 1.2062 - accuracy: 0.6717\n",
      "Epoch 178/200\n",
      "39241/39241 [==============================] - 7s 176us/sample - loss: 1.1925 - accuracy: 0.6743\n",
      "Epoch 179/200\n",
      "39241/39241 [==============================] - 7s 176us/sample - loss: 1.1943 - accuracy: 0.6724\n",
      "Epoch 180/200\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.1968 - accuracy: 0.6756\n",
      "Epoch 181/200\n",
      "39241/39241 [==============================] - 7s 176us/sample - loss: 1.2000 - accuracy: 0.6722\n",
      "Epoch 182/200\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.1944 - accuracy: 0.6712\n",
      "Epoch 183/200\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.1993 - accuracy: 0.6721\n",
      "Epoch 184/200\n",
      "39241/39241 [==============================] - 7s 179us/sample - loss: 1.1895 - accuracy: 0.6748\n",
      "Epoch 185/200\n",
      "39241/39241 [==============================] - 7s 179us/sample - loss: 1.2068 - accuracy: 0.6672\n",
      "Epoch 186/200\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.1965 - accuracy: 0.6748\n",
      "Epoch 187/200\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.1982 - accuracy: 0.6735\n",
      "Epoch 188/200\n",
      "39241/39241 [==============================] - 7s 181us/sample - loss: 1.1971 - accuracy: 0.6734\n",
      "Epoch 189/200\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.2005 - accuracy: 0.6725\n",
      "Epoch 190/200\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.1956 - accuracy: 0.6745\n",
      "Epoch 191/200\n",
      "39241/39241 [==============================] - 7s 173us/sample - loss: 1.1905 - accuracy: 0.6742\n",
      "Epoch 192/200\n",
      "39241/39241 [==============================] - 7s 176us/sample - loss: 1.1893 - accuracy: 0.6767\n",
      "Epoch 193/200\n",
      "39241/39241 [==============================] - 7s 176us/sample - loss: 1.2053 - accuracy: 0.6716\n",
      "Epoch 194/200\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.1842 - accuracy: 0.6762\n",
      "Epoch 195/200\n",
      "39241/39241 [==============================] - 7s 175us/sample - loss: 1.1910 - accuracy: 0.6758\n",
      "Epoch 196/200\n",
      "39241/39241 [==============================] - 7s 178us/sample - loss: 1.1970 - accuracy: 0.6734\n",
      "Epoch 197/200\n",
      "39241/39241 [==============================] - 7s 177us/sample - loss: 1.1851 - accuracy: 0.6782\n",
      "Epoch 198/200\n",
      "39241/39241 [==============================] - 7s 174us/sample - loss: 1.1986 - accuracy: 0.6737\n",
      "Epoch 199/200\n",
      "39241/39241 [==============================] - 7s 179us/sample - loss: 1.1858 - accuracy: 0.6740\n",
      "Epoch 200/200\n",
      "39241/39241 [==============================] - 7s 178us/sample - loss: 1.1833 - accuracy: 0.6778\n",
      "in the market place \n",
      " desmond lets the children lend a hand \n",
      " molly stays\n"
     ]
    }
   ],
   "source": [
    "# Next 200 epochs of training\n",
    "\n",
    "#TB callback\n",
    "log_dir = 'logs/fit/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "tb_callback = tk.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# fit model for 200 epochs\n",
    "model.fit(X, y, batch_size=64, epochs=200, callbacks=[tb_callback])\n",
    "# save model\n",
    "model.save('model.h5')\n",
    "\n",
    "# generate some text\n",
    "seed_text = gen_rand_seq(X, tokenizer)\n",
    "print(gen_text(model, tokenizer, seed_text, max_length, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After another 200 epochs of training (400 total):\n",
    "```\n",
    "in the market place \n",
    " desmond lets the children lend a hand \n",
    " molly stays\n",
    "```\n",
    "Let's see this model in action some more in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sing-a-long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try generating some longer batches of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " i ain't gonna tell you but-a one more time \n",
      " oh , keep your hands ( keep your hands ) off my bay-ee-a-by \n",
      " girl , you get it through your head \n",
      " that boy is mine \n",
      " keep your hands ( keep your hands ) off my bay-ee-a-by \n",
      " girl , you get it through your head \n",
      " that boy is mine \n",
      " keep your hands ( keep your hands ) off my bay-ee-a-by \n",
      " girl , you get it through your head \n",
      " that boy is mine \n",
      " keep your hands ( keep your hands ) off my bay-ee-a-by \n",
      " girl ,\n"
     ]
    }
   ],
   "source": [
    "seed_text = gen_rand_seq(X, tokenizer)\n",
    "print(gen_text(model, tokenizer, seed_text, max_length, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool, definitely sounds vaguely Beatlesesque and there are "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it does with just some random text like: 'hey, hey, sing with me'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey, hey, sing with me \n",
      " \n",
      " i don't wanna kiss you , yeah \n",
      " all i gotta do is act naturally \n",
      " \n",
      " well , i'll bet you i'm gonna be a big star \n",
      " might win an oscar you can never tell \n",
      " i went out to go \n",
      " and can look to me to me \n",
      " and i will sing a lullaby . \n",
      " \n",
      " golden slumbers , \n",
      " fill your eyes \n",
      " smiles await you when you rise \n",
      " sleep pretty darling \n",
      " do not cry \n",
      " so i know that you will plainly see \n",
      " the biggest fool that ever\n"
     ]
    }
   ],
   "source": [
    "seed_text = 'hey, hey, sing with me'\n",
    "print(gen_text(model, tokenizer, seed_text, max_length, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. This looks like a real song. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network really does produce some lyrics that are Beatlesesque. A big part of this was the limited vocabulary of about 2360 words. The limited vocabulary not only outputs words that are already pretty Beatlesesque, but it allowed me to use a reasonably small LSTM (2 layers of 64 hidden nodes each).  \n",
    "\n",
    "I wonder how this would do if we increased the vocabulary space to that of all the words in the lyricsfreak.com dataset. We would potentially be able to get more of a variation in the words (both for input and output). This would make it harder to train but possibly more portable and also allow for the same word embeddings and program to be used for multiple artists. \n",
    "\n",
    "Potential improvements to this network would be:\n",
    "* Increasing the number of nodes or number of LSTM layers\n",
    "* Potentially increasing or decreasing the batch-size\n",
    "* Possibly changing this LSTM to be stateful rather than being stateless\n",
    "* Increasing the length of the input sequence\n",
    "* Training the network to allow for partial sequences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
