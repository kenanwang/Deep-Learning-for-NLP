{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project translates text from German to English. It uses LSTMs, it is trained and tested on a small corpus. \n",
    "\n",
    "Thanks to machinelearningmastery.com from the guide to this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.keras as tk\n",
    "import datetime\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "from unicodedata import normalize\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, RepeatVector, TimeDistributed, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "def load_doc(path):\n",
    "    file = open(path, encoding='utf-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'deu-eng/deu.txt'\n",
    "data = load_doc(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi.\\tHallo!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pairs(data):\n",
    "    lines = data.strip().split('\\n')\n",
    "    pairs = [line.split('\\t')[:2] for line in lines]\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = to_pairs(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hi.', 'Hallo!'],\n",
       " ['Hi.', 'Grüß Gott!'],\n",
       " ['Run!', 'Lauf!'],\n",
       " ['Wow!', 'Potzdonner!'],\n",
       " ['Wow!', 'Donnerwetter!']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the pairs of unuseable text, lowercase and change to ASCII\n",
    "def clean_pairs(pairs):\n",
    "    cleaned=list()\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    for pair in pairs:\n",
    "        clean_pair = list()\n",
    "        for phrase in pair:\n",
    "            phrase = normalize('NFD', phrase).encode('ascii', 'ignore')\n",
    "            phrase = phrase.decode('UTF-8')\n",
    "            phrase = phrase.split()\n",
    "            phrase = [word.lower() for word in phrase]\n",
    "            phrase = [re_punc.sub('', word) for word in phrase]\n",
    "            phrase = [re_print.sub('', word) for word in phrase]\n",
    "            phrase = [word for word in phrase if word.isalpha()]\n",
    "            clean_pair.append(' '.join(phrase))\n",
    "        cleaned.append(clean_pair)\n",
    "    return np.array(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['hi', 'hallo'],\n",
       "       ['hi', 'gru gott'],\n",
       "       ['run', 'lauf'],\n",
       "       ['wow', 'potzdonner'],\n",
       "       ['wow', 'donnerwetter']], dtype='<U527')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_pairs = clean_pairs(pairs)\n",
    "cleaned_pairs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the data\n",
    "pickle.dump(cleaned_pairs, open('cleaned_pairs.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take out a subsample of data for training and testing\n",
    "n_pairs = 10000\n",
    "reduced_data = cleaned_pairs[:n_pairs, :]\n",
    "np.random.shuffle(reduced_data)\n",
    "train, test = reduced_data[:9000], reduced_data[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenizers\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the max length for use in defining model\n",
    "def get_max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng_vocab_dim: 2121\n",
      "eng_length: 5\n",
      "ger_vocab_dim 3381\n",
      "ger_max_length 9\n"
     ]
    }
   ],
   "source": [
    "eng_tokenizer = create_tokenizer(train[:,0])\n",
    "eng_vocab_dim = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = get_max_length(train[:,0])\n",
    "print('eng_vocab_dim:', eng_vocab_dim)\n",
    "print('eng_length:', eng_length)\n",
    "ger_tokenizer = create_tokenizer(train[:,1])\n",
    "ger_vocab_dim = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = get_max_length(train[:,1])\n",
    "print('ger_vocab_dim', ger_vocab_dim)\n",
    "print('ger_max_length', ger_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change sequences to their tokenizer index, and pad the sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    out = tokenizer.texts_to_sequences(lines)\n",
    "    out = pad_sequences(out, maxlen=length, padding='post')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_output(sequences, vocab_dim):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_dim)\n",
    "        ylist.append(encoded)\n",
    "    y = np.array(ylist)\n",
    "    print('before', y.shape)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_dim)\n",
    "    print('after', y.shape)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before (9000, 5, 2121)\n",
      "after (9000, 5, 2121)\n",
      "before (1000, 5, 2121)\n",
      "after (1000, 5, 2121)\n"
     ]
    }
   ],
   "source": [
    "# prepare training data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:,1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:,0])\n",
    "trainY = encode_output(trainY, eng_vocab_dim)\n",
    "# prepare validation data\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:,1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:,0])\n",
    "testY = encode_output(testY, eng_vocab_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, 128, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(Bidirectional(LSTM(n_units)))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units*2, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 9, 128)            432768    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 5, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 5, 2121)           545097    \n",
      "=================================================================\n",
      "Total params: 1,766,345\n",
      "Trainable params: 1,766,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(ger_vocab_dim, eng_vocab_dim, ger_length, eng_length, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      "8960/9000 [============================>.] - ETA: 0s - loss: 4.3039\n",
      "Epoch 00001: val_loss improved from inf to 3.22506, saving model to model.h5\n",
      "9000/9000 [==============================] - 113s 13ms/sample - loss: 4.2997 - val_loss: 3.2251\n",
      "Epoch 2/30\n",
      "8960/9000 [============================>.] - ETA: 0s - loss: 3.2780\n",
      "Epoch 00002: val_loss improved from 3.22506 to 3.05955, saving model to model.h5\n",
      "9000/9000 [==============================] - 53s 6ms/sample - loss: 3.2776 - val_loss: 3.0595\n",
      "Epoch 3/30\n",
      "8960/9000 [============================>.] - ETA: 0s - loss: 3.1319\n",
      "Epoch 00003: val_loss improved from 3.05955 to 2.94987, saving model to model.h5\n",
      "9000/9000 [==============================] - 60s 7ms/sample - loss: 3.1312 - val_loss: 2.9499\n",
      "Epoch 4/30\n",
      "8960/9000 [============================>.] - ETA: 0s - loss: 2.9390\n",
      "Epoch 00004: val_loss improved from 2.94987 to 2.75444, saving model to model.h5\n",
      "9000/9000 [==============================] - 82s 9ms/sample - loss: 2.9386 - val_loss: 2.7544\n",
      "Epoch 5/30\n",
      "8960/9000 [============================>.] - ETA: 0s - loss: 2.6988\n",
      "Epoch 00005: val_loss improved from 2.75444 to 2.57748, saving model to model.h5\n",
      "9000/9000 [==============================] - 66s 7ms/sample - loss: 2.6984 - val_loss: 2.5775\n",
      "Epoch 6/30\n",
      "8960/9000 [============================>.] - ETA: 0s - loss: 2.4913\n",
      "Epoch 00006: val_loss improved from 2.57748 to 2.45836, saving model to model.h5\n",
      "9000/9000 [==============================] - 49s 5ms/sample - loss: 2.4918 - val_loss: 2.4584\n",
      "Epoch 7/30\n",
      "8960/9000 [============================>.] - ETA: 0s - loss: 2.3299\n",
      "Epoch 00007: val_loss improved from 2.45836 to 2.36843, saving model to model.h5\n",
      "9000/9000 [==============================] - 64s 7ms/sample - loss: 2.3297 - val_loss: 2.3684\n",
      "Epoch 8/30\n",
      "8960/9000 [============================>.] - ETA: 0s - loss: 2.1815\n",
      "Epoch 00008: val_loss improved from 2.36843 to 2.27782, saving model to model.h5\n",
      "9000/9000 [==============================] - 75s 8ms/sample - loss: 2.1818 - val_loss: 2.2778\n",
      "Epoch 9/30\n",
      "8960/9000 [============================>.] - ETA: 0s - loss: 2.0404\n",
      "Epoch 00009: val_loss improved from 2.27782 to 2.19430, saving model to model.h5\n",
      "9000/9000 [==============================] - 52s 6ms/sample - loss: 2.0407 - val_loss: 2.1943\n",
      "Epoch 10/30\n",
      "8960/9000 [============================>.] - ETA: 0s - loss: 1.9031\n",
      "Epoch 00010: val_loss improved from 2.19430 to 2.12312, saving model to model.h5\n",
      "9000/9000 [==============================] - 48s 5ms/sample - loss: 1.9032 - val_loss: 2.1231\n",
      "Epoch 11/30\n",
      "8960/9000 [============================>.] - ETA: 0s - loss: 1.7800\n",
      "Epoch 00011: val_loss improved from 2.12312 to 2.05453, saving model to model.h5\n",
      "9000/9000 [==============================] - 51s 6ms/sample - loss: 1.7799 - val_loss: 2.0545\n",
      "Epoch 12/30\n",
      "8960/9000 [============================>.] - ETA: 0s - loss: 1.6636\n",
      "Epoch 00012: val_loss improved from 2.05453 to 2.00121, saving model to model.h5\n",
      "9000/9000 [==============================] - 50s 6ms/sample - loss: 1.6639 - val_loss: 2.0012\n",
      "Epoch 13/30\n",
      "8960/9000 [============================>.] - ETA: 0s - loss: 1.5553\n",
      "Epoch 00013: val_loss improved from 2.00121 to 1.96518, saving model to model.h5\n",
      "9000/9000 [==============================] - 46s 5ms/sample - loss: 1.5555 - val_loss: 1.9652\n",
      "Epoch 14/30\n",
      "8960/9000 [============================>.] - ETA: 0s - loss: 1.4547\n",
      "Epoch 00014: val_loss improved from 1.96518 to 1.91886, saving model to model.h5\n",
      "9000/9000 [==============================] - 48s 5ms/sample - loss: 1.4543 - val_loss: 1.9189\n",
      "Epoch 15/30\n",
      "8960/9000 [============================>.] - ETA: 0s - loss: 1.3571\n",
      "Epoch 00015: val_loss improved from 1.91886 to 1.88586, saving model to model.h5\n",
      "9000/9000 [==============================] - 45s 5ms/sample - loss: 1.3566 - val_loss: 1.8859\n",
      "Epoch 16/30\n",
      "8960/9000 [============================>.] - ETA: 0s - loss: 1.2611\n",
      "Epoch 00016: val_loss improved from 1.88586 to 1.85778, saving model to model.h5\n",
      "9000/9000 [==============================] - 48s 5ms/sample - loss: 1.2609 - val_loss: 1.8578\n",
      "Epoch 17/30\n",
      "8960/9000 [============================>.] - ETA: 0s - loss: 1.1725\n",
      "Epoch 00017: val_loss improved from 1.85778 to 1.81554, saving model to model.h5\n",
      "9000/9000 [==============================] - 46s 5ms/sample - loss: 1.1722 - val_loss: 1.8155\n",
      "Epoch 18/30\n",
      "8960/9000 [============================>.] - ETA: 0s - loss: 1.0851\n",
      "Epoch 00018: val_loss improved from 1.81554 to 1.80638, saving model to model.h5\n",
      "9000/9000 [==============================] - 48s 5ms/sample - loss: 1.0853 - val_loss: 1.8064\n",
      "Epoch 19/30\n",
      "8960/9000 [============================>.] - ETA: 0s - loss: 1.0044\n",
      "Epoch 00019: val_loss improved from 1.80638 to 1.77076, saving model to model.h5\n",
      "9000/9000 [==============================] - 43s 5ms/sample - loss: 1.0049 - val_loss: 1.7708\n",
      "Epoch 20/30\n",
      "8960/9000 [============================>.] - ETA: 0s - loss: 0.9255\n",
      "Epoch 00020: val_loss improved from 1.77076 to 1.75050, saving model to model.h5\n",
      "9000/9000 [==============================] - 49s 5ms/sample - loss: 0.9262 - val_loss: 1.7505\n",
      "Epoch 21/30\n",
      "8960/9000 [============================>.] - ETA: 0s - loss: 0.8529\n",
      "Epoch 00021: val_loss improved from 1.75050 to 1.74760, saving model to model.h5\n",
      "9000/9000 [==============================] - 47s 5ms/sample - loss: 0.8530 - val_loss: 1.7476\n",
      "Epoch 22/30\n",
      "8960/9000 [============================>.] - ETA: 0s - loss: 0.7838\n",
      "Epoch 00022: val_loss improved from 1.74760 to 1.71741, saving model to model.h5\n",
      "9000/9000 [==============================] - 45s 5ms/sample - loss: 0.7841 - val_loss: 1.7174\n",
      "Epoch 23/30\n",
      "8960/9000 [============================>.] - ETA: 0s - loss: 0.7161\n",
      "Epoch 00023: val_loss improved from 1.71741 to 1.70361, saving model to model.h5\n",
      "9000/9000 [==============================] - 42s 5ms/sample - loss: 0.7164 - val_loss: 1.7036\n",
      "Epoch 24/30\n",
      "8960/9000 [============================>.] - ETA: 0s - loss: 0.6591\n",
      "Epoch 00024: val_loss improved from 1.70361 to 1.69800, saving model to model.h5\n",
      "9000/9000 [==============================] - 48s 5ms/sample - loss: 0.6592 - val_loss: 1.6980\n",
      "Epoch 25/30\n",
      "8960/9000 [============================>.] - ETA: 0s - loss: 0.6018\n",
      "Epoch 00025: val_loss improved from 1.69800 to 1.69750, saving model to model.h5\n",
      "9000/9000 [==============================] - 65s 7ms/sample - loss: 0.6016 - val_loss: 1.6975\n",
      "Epoch 26/30\n",
      "8960/9000 [============================>.] - ETA: 0s - loss: 0.5497\n",
      "Epoch 00026: val_loss did not improve from 1.69750\n",
      "9000/9000 [==============================] - 50s 6ms/sample - loss: 0.5499 - val_loss: 1.7041\n",
      "Epoch 27/30\n",
      "8960/9000 [============================>.] - ETA: 0s - loss: 0.5015\n",
      "Epoch 00027: val_loss improved from 1.69750 to 1.68882, saving model to model.h5\n",
      "9000/9000 [==============================] - 48s 5ms/sample - loss: 0.5019 - val_loss: 1.6888\n",
      "Epoch 28/30\n",
      "8960/9000 [============================>.] - ETA: 0s - loss: 0.4600\n",
      "Epoch 00028: val_loss did not improve from 1.68882\n",
      "9000/9000 [==============================] - 44s 5ms/sample - loss: 0.4599 - val_loss: 1.6930\n",
      "Epoch 29/30\n",
      "8960/9000 [============================>.] - ETA: 0s - loss: 0.4188\n",
      "Epoch 00029: val_loss did not improve from 1.68882\n",
      "9000/9000 [==============================] - 42s 5ms/sample - loss: 0.4189 - val_loss: 1.6950\n",
      "Epoch 30/30\n",
      "8960/9000 [============================>.] - ETA: 0s - loss: 0.3861\n",
      "Epoch 00030: val_loss did not improve from 1.68882\n",
      "9000/9000 [==============================] - 43s 5ms/sample - loss: 0.3862 - val_loss: 1.6908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a46d80cd0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set callbacks\n",
    "checkpoint = tk.callbacks.ModelCheckpoint('model.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min') \n",
    "log_dir = 'logs/fit/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "tb_callback = tk.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.fit(trainX, trainY,\n",
    "         epochs=30,\n",
    "         batch_size=64,\n",
    "         validation_data=(testX, testY),\n",
    "         callbacks=[checkpoint, tb_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tk.models.load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to take a model the target tokenizer and a source phrase and predict a translation \n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    ix_to_word = dict((i,w) for w,i in tokenizer.word_index.items())\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    classification = [np.argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in classification:\n",
    "        if i == 0:\n",
    "            break\n",
    "        target.append(ix_to_word[i])\n",
    "    return ' '.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to predict phrases against a corpus of phrases and calculate BLEU Scores for the translations against a reference\n",
    "def evaluate_model(model, sources, raw_dataset):\n",
    "    actual, predicted = list(), list()\n",
    "    for i, source in enumerate(sources):\n",
    "        source = source.reshape((1,source.shape[0]))\n",
    "        translation = predict_sequence(model, eng_tokenizer, source)\n",
    "        raw_target, raw_src = raw_dataset[i]\n",
    "        # for the first ten iteme print the source, the target text, and then the predicted text\n",
    "        if i < 10:\n",
    "            print(f'src={raw_src}, target={raw_target}, predicted={translation}')\n",
    "        actual.append([raw_target.split()])\n",
    "        predicted.append(translation.split())\n",
    "    # calculate the BLEU Scores\n",
    "    BLEU = {\n",
    "        1 : corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)),\n",
    "        2 : corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)),\n",
    "        3 : corpus_bleu(actual, predicted, weights=(0.33, 0.33, 0.33, 0)),\n",
    "        4 : corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    }\n",
    "    return BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src=ich bin beschaftigt, target=im busy, predicted=im busy busy\n",
      "src=krahen sind schwarz, target=crows are black, predicted=crows are black\n",
      "src=gehst du, target=are you going, predicted=are you come\n",
      "src=ich muss es versuchen, target=i have to try, predicted=i can to\n",
      "src=ich werde leben, target=ill live, predicted=ill will\n",
      "src=ich furchte nichts, target=i fear nothing, predicted=i dont nothing\n",
      "src=jetzt fuhle ich es, target=i feel it now, predicted=i i it works\n",
      "src=lassen sie es tom tun, target=let tom do it, predicted=let tom do it\n",
      "src=geht es tom gut, target=is tom well, predicted=did tom ok\n",
      "src=tom mag wein, target=tom likes wine, predicted=tom likes wine\n",
      "Train BLEU: {1: 0.8524946644757209, 2: 0.7921264396531609, 3: 0.6736230757963649, 4: 0.38682243318774046}\n",
      "src=tom ist hellwach, target=toms alert, predicted=toms is\n",
      "src=tom ist weg, target=toms gone, predicted=tom is gone\n",
      "src=du bist ganz reizend, target=youre sweet, predicted=youre great\n",
      "src=er ist nicht versichert, target=hes uninsured, predicted=hes kidding\n",
      "src=ist tom hier, target=is tom here, predicted=is tom here\n",
      "src=ich habe tom geglaubt, target=i believed tom, predicted=i watched tom\n",
      "src=eintritt verboten, target=keep out, predicted=dont hurt\n",
      "src=das kann man in der pfeife rauchen, target=its useless, predicted=its hurt it\n",
      "src=es ist ein hinterhalt, target=its an ambush, predicted=its is\n",
      "src=ich werde schon zurechtkommen, target=ill be fine, predicted=ill manage\n",
      "Test BLEU: {1: 0.5157759775011558, 2: 0.3810208722968439, 3: 0.26187942227213623, 4: 0.12474391941112985}\n"
     ]
    }
   ],
   "source": [
    "train_BLEU = evaluate_model(model, trainX, train)\n",
    "print(f'Train BLEU: {train_BLEU}')\n",
    "test_BLEU = evaluate_model(model, testX, test)\n",
    "print(f'Test BLEU: {test_BLEU}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
